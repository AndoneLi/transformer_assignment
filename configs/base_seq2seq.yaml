task: seq2seq
model:
  d_model: 256
  n_heads: 4
  d_ff: 1024
  n_enc_layers: 4
  n_dec_layers: 4
  dropout: 0.1
  max_len: 256
  src_vocab_size: null
  tgt_vocab_size: null
  attn_type: local
  window_size: 64
  rel_pos: alibi
train:
  batch_size: 32
  lr: 3e-4
  betas: [0.9, 0.95]
  weight_decay: 0.01
  warmup_steps: 400
  max_steps: 3000
  grad_clip: 1.0
  log_every: 50
  eval_every: 300
  use_amp: true
data:
  dataset: iwslt2017
  source_lang: en
  target_lang: de
  limit_train: 10000
  limit_eval: 2000
  num_workers: 2
  data_dir: null
seed: 7
device: auto
